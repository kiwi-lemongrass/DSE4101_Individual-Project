{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are compared based on their forecast performannce on the period from 2024-01-23 to 2025-02-28. (including two ends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forecast Error Metrics (MAE, RMSE, MASE)\n",
    "\n",
    "- Diebold-Mariano Test (for forecast superiority)\n",
    "\n",
    "- Visual Comparison (actual vs. predicted volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the considerations was choosing a recursive estimation scheme over a rolling scheme in forecast estimation. While the latter is better suited for purposes like forecast comparison using DM test (which assumes that the differential is covariance stationary), the recursive estimation scheme is still favoured for practical reasons. This is because by expanding the training set timeframe, it uses more and more data to make later forecasts - parameter uncertainty is reduced, and forecast error may become less variable. Also, parameter estimation uncertainty typically constitutes a tiny fraction of forecast error variance, so reducing it would not noticeably violate stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diebold Mariano (1995) EPA Test\n",
    "def diebold_mariano(actual, forecast1, forecast2, h=3):\n",
    "    \"\"\"\n",
    "    DM test for h-step-ahead forecasts.\n",
    "    H0: Forecasts have equal accuracy.\n",
    "    HA: Forecast 1 is more accurate than Forecast 2.\n",
    "    \"\"\"\n",
    "    # Forecast errors\n",
    "    e1 = actual - forecast1\n",
    "    e2 = actual - forecast2\n",
    "    \n",
    "    # Loss differential (squared errors)\n",
    "    d = e1**2 - e2**2\n",
    "    \n",
    "    # DM statistic (with Newey-West adjustment for autocorrelation)\n",
    "    n = len(d)\n",
    "    d_mean = np.mean(d)\n",
    "    d_var = np.var(d, ddof=1)  # HAC adjustment not shown here (see next section)\n",
    "    dm_stat = d_mean / np.sqrt(d_var / n)\n",
    "    \n",
    "    # Critical value (standard normal for large n)\n",
    "    p_value = 2 * norm.sf(np.abs(dm_stat))  # Two-tailed test\n",
    "    \n",
    "    return dm_stat, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-Ahead Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in [1,3,7,30]:\n",
    "    # load data\n",
    "    garch = pd.read_csv(f'../res/GARCH(1,0,1)_{h}D.csv')['Predicted']\n",
    "    har = pd.read_csv(f'../res/HAR(1, 7, 30)_{h}D.csv')['Pred']\n",
    "    garch_svm = pd.read_csv(f'../res/GARCH-SVM_{h}D.csv')['Predicted']\n",
    "    actual = pd.read_csv(f'../res/lnRV_test_{h}D.csv')['lnRV']\n",
    "\n",
    "    # Mean Absolute Error (MASE)\n",
    "    mae_garch = mean_absolute_error(actual, garch)\n",
    "    mae_har = mean_absolute_error(actual, har)\n",
    "    mae_garch_svm = mean_absolute_error(actual, garch_svm)\n",
    "\n",
    "    # Mean Absolute Error (MASE)\n",
    "    mae_garch = mean_absolute_error(actual, garch)\n",
    "    mae_har = mean_absolute_error(actual, har)\n",
    "    mae_garch_svm = mean_absolute_error(actual, garch_svm)\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse_garch = np.sqrt(mean_squared_error(actual, garch))\n",
    "    rmse_har = np.sqrt(mean_squared_error(actual, har))\n",
    "    rmse_garch_svm = np.sqrt(mean_squared_error(actual, garch_svm))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mape_garch = mean_absolute_percentage_error(actual, garch) * 100\n",
    "    mape_har = mean_absolute_percentage_error(actual, har) * 100\n",
    "    mape_garch_svm = mean_absolute_percentage_error(actual, garch_svm) * 100\n",
    "\n",
    "    # Mean Absolute Scaled Error (MASE) - Lagged realized vol as benchmark\n",
    "    naive_forecast = np.roll(actual, h)\n",
    "    naive_forecast[:h] = np.nan  # Set the first h values to NaN\n",
    "    mase_garch = mean_absolute_error(actual[h:], garch[h:]) / mean_absolute_error(actual[h:], naive_forecast[h:])\n",
    "    mase_har = mean_absolute_error(actual[h:], har[h:]) / mean_absolute_error(actual[h:], naive_forecast[h:])\n",
    "    mase_garch_svm = mean_absolute_error(actual[h:], garch_svm[h:]) / mean_absolute_error(actual[h:], naive_forecast[h:])\n",
    "\n",
    "    # R-squared\n",
    "    r2_garch = r2_score(actual, garch)\n",
    "    r2_har = r2_score(actual, har)\n",
    "    r2_garch_svm = r2_score(actual, garch_svm)\n",
    "\n",
    "    # Diebold-Mariano test\n",
    "    dm_stat_garch_har, p_value_garch_har = diebold_mariano(actual, garch, har)\n",
    "    dm_stat_garch_svm, p_value_garch_svm = diebold_mariano(actual, garch, garch_svm)\n",
    "    dm_stat_har_svm, p_value_har_svm = diebold_mariano(actual, har, garch_svm)\n",
    "\n",
    "    # save results to csv\n",
    "    results = pd.DataFrame({\n",
    "        'Metric': ['MAE', 'RMSE', 'MAPE', 'MASE', 'R-squared'],\n",
    "        'GARCH': [mae_garch, rmse_garch, mape_garch, mase_garch, r2_garch],\n",
    "        'HAR': [mae_har, rmse_har, mape_har, mase_har, r2_har],\n",
    "        'GARCH-SVM': [mae_garch_svm, rmse_garch_svm, mape_garch_svm, mase_garch_svm, r2_garch_svm],\n",
    "    })\n",
    "    results.to_csv(f'../eval/metrics_{h}D.csv', index=False)\n",
    "\n",
    "    # save DM test results to csv\n",
    "    dm_results = pd.DataFrame({\n",
    "        'Comparison': ['GARCH vs HAR', 'GARCH vs GARCH-SVM', 'HAR vs GARCH-SVM'],\n",
    "        'DM Statistic': [dm_stat_garch_har, dm_stat_garch_svm, dm_stat_har_svm],\n",
    "        'p-value': [p_value_garch_har, p_value_garch_svm, p_value_har_svm]\n",
    "    })\n",
    "    dm_results.to_csv(f'../eval/DM_test_{h}D.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
